<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Eight million pixels and counting - graphics</title><link href="https://nical.github.io/" rel="alternate"></link><link href="https://nical.github.io/feeds/graphics.atom.xml" rel="self"></link><id>https://nical.github.io/</id><updated>2018-10-28T00:00:00+02:00</updated><entry><title>2d graphics in Rust discussion - A look at GPU memory management</title><link href="https://nical.github.io/posts/rust-2d-graphics-02.html" rel="alternate"></link><published>2018-10-28T00:00:00+02:00</published><updated>2018-10-28T00:00:00+02:00</updated><author><name>Nical</name></author><id>tag:nical.github.io,2018-10-28:/posts/rust-2d-graphics-02.html</id><summary type="html">&lt;p&gt;In this post I'll write about an piece of the low level details of an hypothetical rust 2d graphics crate built on top of &lt;a href="https://github.com/gfx-rs/gfx"&gt;gfx-hal&lt;/a&gt;. Gfx provides a vulkan-like interface implemented on top of vulkan, d3d12, metal or flavors of OpenGL. just like the &lt;a href="https://nical.github.io/posts/rust-2d-graphics-01.html"&gt;previous post&lt;/a&gt; this is in the â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post I'll write about an piece of the low level details of an hypothetical rust 2d graphics crate built on top of &lt;a href="https://github.com/gfx-rs/gfx"&gt;gfx-hal&lt;/a&gt;. Gfx provides a vulkan-like interface implemented on top of vulkan, d3d12, metal or flavors of OpenGL. just like the &lt;a href="https://nical.github.io/posts/rust-2d-graphics-01.html"&gt;previous post&lt;/a&gt; this is in the context of recent discussions about a 2d graphics crate in rust.&lt;/p&gt;
&lt;p&gt;I won't actually write much about 2d graphics specific things this time, because a lot of these low level concerns are agnostic to whether the rendered content is in two or three dimensions. I'll mostly focus on memory management and command submission.&lt;/p&gt;
&lt;p&gt;This low level thing I am going to talk about isn't a 2d graphics API that most users would play with but rather a base component on top of which various rendering techniques could be implemented (for example using &lt;a href="https://github.com/nical/lyon"&gt;lyon&lt;/a&gt;, &lt;a href="https://github.com/pcwalton/pathfinder"&gt;pathfinder&lt;/a&gt; or some other approach), and this base component would be independent of these rendering techniques.&lt;/p&gt;
&lt;h2&gt;Organizing the input of drawing commands on the GPU&lt;/h2&gt;
&lt;p&gt;Let's have a look at &lt;a href="https://github.com/servo/webrender"&gt;WebRender&lt;/a&gt;. Most drawing primitives in WebRender use very simple geometry (axis-aligned rectangles) with shaders of varying complexity that compute directly on the GPU what happens to the pixels covered by the geometry. If you've ever done serious work with GPU, one of your primary concerns most likely was to batch rendering commands and avoid state changes. To render a few thousand rectangles, it would be terribly inefficient to have a loop on the CPU that sets some parameters and kicks a drawing command for each rectangle. So what we do is write a lot of parameters into buffers which are sent to the GPU, and kick a few drawing commands, that will each render maybe thousands of rectangles in one go using instancing.
The drawing parameters we write into the buffer contain information such as the position and size of the rectangle in layout space, a transformation to go from layout space to screen space, some flags about whether some anti-aliasing must be done, the positions and sizes of some source images in a texture atlas if need be (for example to apply a texture or a mask), the z-index of the primitive to write into the depth buffer, etc.
The vertex shader uses an instance id to find the right information in a per-instance parameter buffer, transforms the geometry (a unit quad) into the right rectangle on screen, forwards some data to the fragment shader and the latter executes the per pixel logic (read from the source textures, apply some effect, write output color, etc.).&lt;/p&gt;
&lt;p&gt;&lt;img alt="gpu memory organization example" src="https://nical.github.io/images/gpu-mem-01.svg"&gt;&lt;/p&gt;
&lt;p&gt;This approach is fairly generic, not particular to WebRender (a lot of games do this sort of thing) and works quite well.
What's interesting here is that if you get to write the shaders yourself you have a lot of flexibility in how the input data is organized in GPU memory. You can put all of the parameters for a given instance contiguous together or add levels of indirection, share some common parameters among many instances or even devise your own compression scheme for your data. All that matters is for the CPU code to know how to writes bytes and the shader to know how to find the data in GPU memory and interpret it the right way, and then it is up to you to decide what trade-off to make about simplicity, memory usage and data locality.&lt;/p&gt;
&lt;p&gt;I like to think of the problem of submitting commands to the GPU as some sort of encoding/decoding problem. Got data on one side, some binary representation in GPU buffers and need to read that correctly in the shader. As such, submitting commands is very tied to GPU memory management since it amounts to writing the correct information in the right place.
In the purpose of a 2d graphics library, what kind of representation would we want? Well we don't know because the shape the data you write into GPU memory may depend on the details of the rendering technique itself and I mentioned earlier that I want this low level component to be agnostic to that. Some other layer above will deal with it and for now we can focus providing a way to write data into these GPU buffers.&lt;/p&gt;
&lt;h2&gt;Dealing with GPU memory allocation&lt;/h2&gt;
&lt;p&gt;Let's first have a look at some properties I want from a low level GPU memory management system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be able to write commands from several threads in parallel.&lt;/li&gt;
&lt;li&gt;Be able to separate memory that is rarely updated from memory that we update often in order to minimize transfer and choose the right kind of memory heap.&lt;/li&gt;
&lt;li&gt;The system should be embeddable in another application that uses gfx.&lt;/li&gt;
&lt;li&gt;Ideally the shader should not have to care about whether a particular property is static or dynamic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Legacy graphics APIs had a lot of memory management magic done in the driver. Modern APIs such as vulkan move this responsibility to the user of the API (us). Simply allocating a lot of small memory segments has a lot overhead. &lt;a href="https://youtu.be/zSG6dPq57P8?t=1163"&gt;AMD's recommendation&lt;/a&gt; is to allocate large chunks of memory segments of 256 MiB and manage sub-allocations within these coarse chunks, others talk about 128 MiB blocks, &lt;a href="https://developer.nvidia.com/vulkan-memory-management"&gt;nVidia's recommendation&lt;/a&gt; is to manage all allocations out of as few as possible large contiguous memory chunks.&lt;/p&gt;
&lt;p&gt;So we can decide to have these large memory blocks with various allocation schemes within them, from a simple bump allocator to a general purpose fully-featured allocator.&lt;/p&gt;
&lt;p&gt;For per-frame data which is overwritten each time, a bump allocator looks like a good fit: It is simple, fast and we can easily write a lock-free one that is usable in parallel. For completely static (as in very long-lived) allocations, bump allocators can also work well since we don't need to worry about fragmentation. The area in the middle is a bit tricker. I don't have a precise definitive allocation strategy to provide here, other than that in my opinion, nesting simple and efficient allocators is the way to go. Group memory allocations into blocks of the same lifetime that get deallocated in one go, and try to keep the complicated parts of memory managements for fewer coarser chunks, also try to group allocations by update frequency. In other words, use specific, simple and deterministic allocation strategies rather than relying on a single complicated one-size-fits-all allocation interface like if you had jemalloc for your GPU. Making such a general purpose allocator is for one very hard, and ensuring good and deterministic performance characteristics is even harder.&lt;/p&gt;
&lt;p&gt;This idea of nested allocators is a good fit for something that can be embedded into another graphics engine. For example the coarse allocations could be requested to the embedder and the 2d renderer would manage its own allocations inside of them.
For multi-threading this is also good: Some thread-safe allocator can allocate chunks in parallel, and these chunks could be managed independently in single-thread fashion but each on their single thread by a nested allocator that is hard to make thread-safe.&lt;/p&gt;
&lt;p&gt;Some allocations are best managed manually (explicit allocation and deallocation), while for some other things, using a cache can be more interesting: Each frame, the CPU side requests that the allocations that are used within the cache stay alive, and if an allocation isn't used for a long time, the cache implicitly deallocates it. The next time the allocation is requested, the cache will politely tell the source of the request that the allocation doesn't exist anymore, a new allocation is made and the data is transfered again. A lot the GPU memory management in WebRender is done that way. Having the right heuristics about when to expire cache entries is not always simple.&lt;/p&gt;
&lt;p&gt;So far I talked about allocations in the sense of figuring out how many bytes at which offset to reserve in GPU memory for this and that, but we also want to write into these allocations. Often times people think of allocating and writing into memory as single thing. When writing &lt;code&gt;let five = Box::new(5)&lt;/code&gt; you both ask the allocator to figure out where the value will be and write the value in memory. But sending data to the GPU isn't that simple. In general you can't assume that the memory you are writing to on the CPU is the one that gets read in the shaders. There are several memory heaps with different characteristics (CPU-visible, GPU-visible, fast/slow to read/write on the CPU/GPU, etc.). In practice this means that for a lot things the data is first written into a staging buffer that is CPU-visible, then copied from there into memory that is fast to read from the shaders.&lt;/p&gt;
&lt;p&gt;&lt;img alt="staging buffer" src="https://nical.github.io/images/gpu-mem-02.svg"&gt;&lt;/p&gt;
&lt;p&gt;One strategy could be to work with a large GPU buffer for the shader to read and generally smaller staging buffers into which we write only the parts that have changed. In this scheme the shader gets to read from a single contiguous buffer containing static and dynamic data alike and doesn't have to be aware of that. This is at the cost of copying from the staging buffer for data that we now will only read for a single frame. If a lot of data needs to be updated and read only once each frame, using another type of GPU memory heap that is both accessible to the CPU and the GPU, is slower to read in the shader but depending on how the data is read it might still be faster than the copy of the staging buffer. &lt;a href="https://youtu.be/zSG6dPq57P8?t=991"&gt;An example&lt;/a&gt; given for this type of memory heap is particle positions in a game, where by definition we know all of it will change each frame. The downside is that the shader can't pretend it is in a unified address space with the rest, so it isn't agnostic to what is animated and what is not. This might not be a good fit for cases where, say, some of the positions are animated while others are not and it all goes through the same code path in the shader.
Another thing to be careful about is that some types of memory heaps use write-combined memory which is ideal to fill the staging buffer but can perform poorly if we don't pay attention to how we write into it. So we have to allocate full aligned multiple-of-cacheline sized chunks and avoid random access.&lt;/p&gt;
&lt;h2&gt;What's next?&lt;/h2&gt;
&lt;p&gt;Phew. That was a lot of words just for pushing some bytes to the GPU! I merely presented some challenges, proposed certain directions from very far away and haven't even really talked about rendering (the fancy tricks that goes into those shaders, generating the geometry and all).&lt;/p&gt;
&lt;p&gt;But I believe this is an important part of the foundation. It is built upon mostly independent pieces and that's already some code that needs writing. Fortunately, this weekend was the first in a while that I had time to sit down and do that so I took one of gfx-hal examples and started writing a few things around it. So far I only have a simple &lt;a href="https://github.com/nical/lyon/blob/371af479cd743c102487835a74299ac50967cadd/renderer/src/allocator.rs#L4"&gt;bump allocator&lt;/a&gt;, some &lt;a href="https://github.com/nical/lyon/blob/371af479cd743c102487835a74299ac50967cadd/renderer/src/writer.rs#L93"&gt;glue to use it and write into memory&lt;/a&gt; from multiple threads, a simple and dumb &lt;a href="https://github.com/nical/lyon/blob/371af479cd743c102487835a74299ac50967cadd/renderer/src/allocator.rs#L54"&gt;retained allocator&lt;/a&gt; for coarse allocations, some &lt;a href="https://github.com/nical/lyon/blob/371af479cd743c102487835a74299ac50967cadd/renderer/src/gfx.rs"&gt;reexports of the gfx types&lt;/a&gt; without generics all over the place (since the backend is selected with feature flag), and various &lt;a href="https://github.com/nical/lyon/blob/371af479cd743c102487835a74299ac50967cadd/renderer/src/lib.rs#L99"&gt;small utilities&lt;/a&gt;. That's not much, I'm still spending time getting familiar with gfx, but that's a humble start.&lt;/p&gt;
&lt;p&gt;I'll end this post with hand-wavey overview of how I see this stuff fitting into the bigger picture:&lt;/p&gt;
&lt;p&gt;&lt;img alt="boring architecture diagram" src="https://nical.github.io/images/gpu-mem-03.svg"&gt;&lt;/p&gt;
&lt;p&gt;From bottom to top there is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A gfx-device that is owned by an embedder.&lt;/li&gt;
&lt;li&gt;The embedder can be the default one or some glue to integrate with a specific engine or app).&lt;/li&gt;
&lt;li&gt;The low level components contain anything that is mostly independent of the rendering technique, for example a lot of the memory management code that I wrote about in this post. The goal of the low level components is to make it easy to write efficient rendering components without having each of them reinvent the common pieces. Rendering components do the more interesting stuff: They register passes, shaders, and implement fun rendering techniques. An example of rendering component could be a glyph renderer using pathfinder. Another one could be a tessellated polygon renderer that uses lyon, another one could be something that is optimized for rendering many axis-aligned rectangles as is common for UIs. Rendering components inform the command submission system which draw calls are order-independent as well as some global requirements such as depth/stencil behavior, etc. This lets the low level pieces figure out an efficient batching strategy.&lt;/li&gt;
&lt;li&gt;You see on top of that several "API" boxes. These take care of high level logic and providing user facing APIs. Each API would have to make different tradeoffs and focus on different use cases, picking whatever rendering components they want to use.
As much as possible I would like to avoid compromises outside of the "API" boxes which will be potentially high level abstractions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm intentionally describing this in very broad terms. The idea behind separating things this way is to be able to experiment with various rendering techniques without starting from scratch each time, and also aim for an extendable architecture if things work out. There is a lot of pieces missing and the boundaries will move as real code gets written but this sets a general direction I am happy with for a low level GPU based 2d graphics library.&lt;/p&gt;</content></entry><entry><title>Following up on the 2d graphics in Rust discussion</title><link href="https://nical.github.io/posts/rust-2d-graphics-01.html" rel="alternate"></link><published>2018-10-14T00:00:00+02:00</published><updated>2018-10-14T00:00:00+02:00</updated><author><name>Nical</name></author><id>tag:nical.github.io,2018-10-14:/posts/rust-2d-graphics-01.html</id><summary type="html">&lt;p&gt;Raph Levien recently published &lt;a href="https://raphlinus.github.io/rust/graphics/2018/10/11/2d-graphics.html"&gt;A crate I want: 2d graphics&lt;/a&gt; on his blog, which started some interesting discussions &lt;a href="https://www.reddit.com/r/rust/comments/9nhhh8/a_crate_i_want_2d_graphics/"&gt;on reddit&lt;/a&gt;. At the same time there is a nascent discussion on the &lt;a href="https://github.com/draw2d/rfcs/issues/1"&gt;draw2d&lt;/a&gt; repository (which doesn't have any code at this point) about a potential 2d graphics crate.&lt;/p&gt;
&lt;p&gt;These discussions contain â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Raph Levien recently published &lt;a href="https://raphlinus.github.io/rust/graphics/2018/10/11/2d-graphics.html"&gt;A crate I want: 2d graphics&lt;/a&gt; on his blog, which started some interesting discussions &lt;a href="https://www.reddit.com/r/rust/comments/9nhhh8/a_crate_i_want_2d_graphics/"&gt;on reddit&lt;/a&gt;. At the same time there is a nascent discussion on the &lt;a href="https://github.com/draw2d/rfcs/issues/1"&gt;draw2d&lt;/a&gt; repository (which doesn't have any code at this point) about a potential 2d graphics crate.&lt;/p&gt;
&lt;p&gt;These discussions contain a lot of interesting comments. One notable thing is that "2d graphics" means different things to different people. Some people want to render UIs, some people want to make games with rich vector graphics, some people want to be able to render complex but static SVG documents, some people want something designed for modern GPUs, some people want a library that can be used without a GPU, some want it low level, other want it high level, &lt;em&gt;et caetera&lt;/em&gt;.
Having worked with and on different approaches to 2d graphics rendering, I am convinced that 2d graphics isn't a single problem with an established solution, but rather collection of different problems which are best solved with very different solutions.&lt;/p&gt;
&lt;p&gt;Because of that I would like this discussion to branch into several specific topics. In this post I'll talk about 2d graphics for the web and for user interfaces (a case could be made about separating the two of course), to a large extent with the intent to underline how specific the needs of these use cases are. Before I do that I'll write a few words about "canvas-like" graphics APIs.&lt;/p&gt;
&lt;h2&gt;Canvas, Cairo, Skia, QPainter - The immediate mode painting model&lt;/h2&gt;
&lt;p&gt;&lt;img alt="canvas-like APIs" src="https://nical.github.io/images/canvas-api.svg"&gt;&lt;/p&gt;
&lt;p&gt;A lot of the most common 2d graphics APIs look somewhat similar: We have a context object which holds various drawing states like the current transform, pattern, or clip, we have a way to describe paths in post-script fashion (for example &lt;code&gt;context.move_to&lt;/code&gt;, &lt;code&gt;context.line_to&lt;/code&gt;, or if we are lucky, actual path objects that can be retained and reused). The general mental model is that you specify each drawing operation in back to front order and pixels are being painted immediately at each operation (or this is what the API pretends) but in practice drawing might be deferred so that the underlying implementation can take advantage of knowing about all of the elements and perform some global optimizations. For example CPU backends tend paint each drawing command individually, but GPU implementations pretty much need to be able to perform global optimizations in order to get acceptable performance. Another important aspect of these APIs is that they don't retain a description of the scene, which means that in order to render interactive content, you will submit the commands again each frame with some modifications for the parts that are changing. A common optimization when using this drawing model is for the user to track which region of the final image has changed and only redraw that part (in Firefox we call it invalidation).&lt;/p&gt;
&lt;p&gt;Generally, implementing an efficient GPU backend for a pre-existing canvas-like API has proven to be difficult (doable but difficult). A lot of these APIs were designed before GPUs existed or became what they are now, and often a lot of expensive transformations need to happen between the API and the underlying GPU command submissions.&lt;/p&gt;
&lt;h2&gt;Compositors&lt;/h2&gt;
&lt;p&gt;When motion comes into play (through animations or scrolling), re-rendering rectangles, paths, text and whatever else is moving at a high frame rate can become expensive with these drawing models. A very common solution to this problem is to group elements that tend to move together and paint them into retained surfaces that are often called layers (at least in the world of web browsers). Scrolling then becomes a matter of moving one or several layers and compositing them together to form the resulting image. This compositing operation is the job of... the compositor. Compositing within an application is very similar to compositing for a window manager, to the point where all modern window managers now provide APIs to let applications delegate the work of compositing their layers to the window manager, which avoids one compositing step, saves cycles, memory bandwidth and power. It is somewhat painful that all platforms expose this functionality in subtly different ways, but I think that it would be a mistake for any new UI toolkit to ignore it if they have a concept of retained surfaces.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gecko compositor" src="https://nical.github.io/images/gecko-compositor.png"&gt;&lt;/p&gt;
&lt;h2&gt;Rendering web pages and UIs&lt;/h2&gt;
&lt;p&gt;By now I already gave away some of the details about how a lot of web browsers and UI toolkits work: an immediate mode painting abstraction, on top of which an invalidation system and a compositor were implemented to paper over the difficulty of rendering at 60 frames per second (without draining too much power). This is pretty much how Firefox (and most other browsers), Gtk and Qt have been historically approaching 2d graphics. This model is also to some extent something that Firefox, Gtk and Qt have or are in the process of going away from. I'll get to that in a moment, but before that I want to look at what drawing primitives are important for a web browser and a UI toolkit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text. Oh text. Text is of course very important in this context and &lt;em&gt;must&lt;/em&gt; have a particular look in each platform because a lot of users are bothered when hinting or anti-aliasing in an app doesn't match the look of the rest of the platform. In practice this means a lot of graphics libraries go through great lengths and pain to use the font rendering system of each platform they support. Text is hard. I wont get into these details but the people who implement the myriad of things that happen between loading a font file and the letters of this blog post showing up on your screen are heroes (no self-congratulations, I only touched very small portions of that). Patrick Walton is working on the &lt;a href="https://github.com/pcwalton/font-kit"&gt;font-kit crate&lt;/a&gt; which will hopefully erase some of the pain of dealing with font on each platform. I haven't used it yet but it's something to keep an eye on. Unless you are on a very high dpi screen, sub-pixel positioning and anti-aliasing make text a lot easier and nicer to read. One might decide to only target high dpi displays and punt on these features, but I wouldn't call that an easy decision and it will rule out some use case for sure.&lt;/li&gt;
&lt;li&gt;Simple shapes: mostly axis aligned rectangles and rounded rectangles, filled and their outlines.&lt;/li&gt;
&lt;li&gt;Very complex clipping scenarios: That one might not be necessary for some UI toolkits, but I'll mention it because clipping is a hard problem when your job is to render web pages. In any case it took quite a bit of trial and error to get that work with all use cases in WebRender.&lt;/li&gt;
&lt;li&gt;A few effects like drop shadows and blurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice how I didn't mention arbitrary paths? Of course these need to be implemented in a web browser but without them you can already render a large portion of the web. Any modern UI toolkit also needs to render arbitrary vector graphics to provide resolution independent icons and some types of UI widgets, but the need doesn't compare to the heaps of rectangles and text that compose UIs and web pages.
Since you still need to support both arbitrary vector graphics and these specific simple shapes, you can simply focus on rendering the complex cases and you'll get the simple case for free, right? Except that in doing so, by not making the overwhelmingly most common case a first class citizen of the system a lot of potential for performance optimization would be lost.
Today, WebRender doesn't know how to render arbitrary shapes. It's main primitives are (rounded) rectangles and more complex shapes just go through a fall-back mechanism that is part of Gecko and provides WebRender with the resulting content already rasterized. We'll eventually implement path rendering on the GPU but the current situation is good enough for us to ship WebRender first with the fall-back.
My understanding is that the situation is similar with Gsk (Gtk's next generation rendering infrastructure): arbitrary vector graphics are rasterized using Cairo on the CPU while great care goes into rasterizing rectangles on the GPU.&lt;/p&gt;
&lt;p&gt;Interestingly, for a lot of people path rendering is the central theme when thinking about 2d graphics. My point isn't to state what features are more important than others for a 2d graphics library, but really to stress how different the use cases are and that Rust does not need a single 2d graphics crate, but several, each with different goals and trade-offs.&lt;/p&gt;
&lt;p&gt;Speaking of focus, I will now attempt to get my chaotic train of thought together and come back to the topic of web pages and UI toolkit I was writing about a few lines earlier.&lt;/p&gt;
&lt;h2&gt;Animation&lt;/h2&gt;
&lt;p&gt;I already mentioned motion. It is important enough that browsers and UI toolkits went from a simple "draw each element directly into the pixels of the window" model to embedding a compositor and/or relying on the window manager's compositor to accelerate moving things around. In my opinion, not taking motion/animation into account is one of the biggest mistakes a new graphics API design can make in he context of UIs (and in many other use cases). Frame N+1 is usually very similar to frame N, and taking advantage of this is key to get good performance and battery usage in rich and interactive experiences. There are different ways in which two frames can be similar. For example a few elements could be added, or all elements could exist in the two frames with different positions. For some APIs the latter might be considered as being two totally different scenes because all elements have changed, for other APIs that retain the structure of the scene it could just be thought of as the same frame with a different array of animation parameters. Whether the latter is superior to the former really depends on what you think the common use cases will be what is worth optimizing for.
WebRender treats updates with several levels of priority. Scrolling for example &lt;em&gt;must&lt;/em&gt; be able to operate at 60 frames per second, some larger changes to the structure of the scene can be more expensive to operate and are being carried asynchronously to ensure scrolling remains smooth. This is one of the many solutions to the common problem of having to deal with operations that could blow the frame budget. Another way could be to allow expensive work to be done asynchronously by users of the API prior to submitting rendering instructions. In any case I think that it is important to identify these bottlenecks and design the system in a way that prevents them from hurting frame rate.&lt;/p&gt;
&lt;p&gt;In term of API, WebRender is much higher level than a traditional canvas-like API. Instead of building a single frame by submitting drawing commands, consumers of the API build a retained representation made of nested "display lists", and several frames will be rendered from this representation when animated properties of these display lists change (for example scrolling, or other types of animations). By squinting really hard one might see the display list building API as a sort of canvas-like API that builds a scene instead of an image, with an added notion of animated properties. But the display list building API itself doesn't have the generalist and flexible graphics API look of a Cairo or a Skia, instead focusing on describing positioned CSS primitives.&lt;/p&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;At this point my plan was to delve into how WebRender approaches rendering on the GPU in more details and explain in what ways it fundamentally differs from Firefox's previous approach, but I already wrote too many words for a single post. To be honest, I think that WebRender already has the potential to fill the UI rendering niche. Some useful features are missing (for example a full software fallback), but these features are wanted in the long term anyway, so better add them (pull requests are welcome!) than try to replicate the colossal amount of effort that already went into the project.&lt;/p&gt;
&lt;p&gt;Let this post be about me trying to convince you that rendering UIs comes with very specific design constraints and requirements, and that other niches which I failed to talk about would benefit from very different solutions. Whenever I have time to sit down and write another post I'll try to explain what I think would be a good direction for an hypothetical low-level crate focused on path rendering on the GPU with an eye for complex and dynamic scenes. Something very different in goal and scope from WebRender (and in some ways complementary). My hope is that I can stir some of the potential "Let's get together and make a 2d graphics crate" vibe towards that vision.&lt;/p&gt;</content></entry></feed>